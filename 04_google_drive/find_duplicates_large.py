import os
import hashlib
from tqdm import tqdm
from datetime import datetime

# æ”¯æ´çš„åœ–ç‰‡èˆ‡å½±ç‰‡å‰¯æª”å
IMAGE_EXTS = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'}
VIDEO_EXTS = {'.mp4', '.mov', '.avi', '.mkv', '.flv', '.wmv', '.mpeg', '.webm'}
ALL_EXTS = IMAGE_EXTS.union(VIDEO_EXTS)

def get_file_hash(file_path, chunk_size=4096):
    """è¨ˆç®—æª”æ¡ˆ SHA256 é›œæ¹Š"""
    hasher = hashlib.sha256()
    try:
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(chunk_size), b''):
                hasher.update(chunk)
        return hasher.hexdigest()
    except Exception as e:
        print(f"ç„¡æ³•è®€å– {file_path}ï¼š{e}")
        return None

def find_duplicates_and_largest(folder):
    """å°‹æ‰¾é‡è¤‡çš„æª”æ¡ˆï¼Œä¸¦åŒæ™‚æ‰¾å‡ºæª”æ¡ˆæœ€å¤§çš„20å€‹èˆ‡çµ±è¨ˆè³‡è¨Š"""
    hashes = {}
    duplicates = []
    file_list = []
    total_size = 0
    duplicate_size = 0

    for root, _, files in os.walk(folder):
        for filename in files:
            ext = os.path.splitext(filename)[1].lower()
            if ext in ALL_EXTS:
                full_path = os.path.join(root, filename)
                try:
                    size = os.path.getsize(full_path)
                    total_size += size
                    file_list.append((full_path, size))
                except Exception as e:
                    print(f"âŒ ç„¡æ³•å–å¾—å¤§å° {full_path}: {e}")

    for full_path, size in tqdm(file_list, desc="è™•ç†é€²åº¦"):
        file_hash = get_file_hash(full_path)
        if file_hash:
            if file_hash in hashes:
                duplicates.append((hashes[file_hash], full_path, size))
                duplicate_size += size
            else:
                hashes[file_hash] = full_path

    top_20_largest = sorted(file_list, key=lambda x: x[1], reverse=True)[:20]

    stats = {
        "total_files": len(file_list),
        "total_size": total_size,
        "duplicate_count": len(duplicates),
        "duplicate_size": duplicate_size
    }

    return duplicates, top_20_largest, stats

def write_output(duplicates, largest_files, stats, timestamp):
    dup_file = f'{timestamp}_duplicates_files.txt'
    size_file = f'{timestamp}_largest_files.txt'

    with open(dup_file, 'w', encoding='utf-8') as f:
        for original, duplicate, size in duplicates:
            f.write(f"åŸå§‹æª”æ¡ˆ: {original}\né‡è¤‡æª”æ¡ˆ: {duplicate}\næª”æ¡ˆå¤§å°: {size / (1024 * 1024):.2f} MB\n\n")
    print(f"âœ… å·²å¯«å…¥ {len(duplicates)} ç­†é‡è¤‡è³‡æ–™åˆ° {dup_file}")

    with open(size_file, 'w', encoding='utf-8') as f:
        f.write("ğŸ“Š çµ±è¨ˆè³‡è¨Š\n")
        f.write(f"æƒææª”æ¡ˆç¸½æ•¸: {stats['total_files']}\n")
        f.write(f"æƒæç¸½å¤§å°: {stats['total_size'] / (1024 * 1024):.2f} MB\n")
        f.write(f"é‡è¤‡æª”æ¡ˆæ•¸é‡: {stats['duplicate_count']}\n")
        f.write(f"å¯é‡‹æ”¾ç©ºé–“ç¸½è¨ˆ: {stats['duplicate_size'] / (1024 * 1024):.2f} MB\n")
        f.write("\nğŸ“ å‰ 20 å¤§æª”æ¡ˆï¼š\n")
        for path, size in largest_files:
            f.write(f"{path} ({size / (1024 * 1024):.2f} MB)\n")
    print(f"âœ… å·²å¯«å…¥å‰ 20 å¤§æª”æ¡ˆèˆ‡çµ±è¨ˆè³‡è¨Šåˆ° {size_file}")

if __name__ == "__main__":
    folder_to_scan = input("è¯·è¾“å…¥è¦æœç´¢çš„æ–‡ä»¶å¤¹è·¯å¾„: ").strip()
    if not os.path.isdir(folder_to_scan):
        print("âŒ æŒ‡å®šçš„è·¯å¾‘ä¸å­˜åœ¨æˆ–ä¸æ˜¯è³‡æ–™å¤¾ã€‚")
    else:
        timestamp = datetime.now().strftime('%m%d_%H%M')
        dup_files, largest_files, stats = find_duplicates_and_largest(folder_to_scan)
        write_output(dup_files, largest_files, stats, timestamp)
