import os
import hashlib
from tqdm import tqdm
from datetime import datetime
from tkinter import Tk, filedialog

# æ”¯æ´çš„åœ–ç‰‡èˆ‡å½±ç‰‡å‰¯æª”å
IMAGE_EXTS = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'}
VIDEO_EXTS = {'.mp4', '.mov', '.avi', '.mkv', '.flv', '.wmv', '.mpeg', '.webm'}
ALL_EXTS = IMAGE_EXTS.union(VIDEO_EXTS)

def get_file_hash(file_path, chunk_size=4096):
    """è¨ˆç®—æª”æ¡ˆ SHA256 é›œæ¹Š"""
    hasher = hashlib.sha256()
    try:
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(chunk_size), b''):
                hasher.update(chunk)
        return hasher.hexdigest()
    except Exception as e:
        print(f"ç„¡æ³•è®€å– {file_path}ï¼š{e}")
        return None

def find_duplicates_and_largest(folder):
    hashes = {}
    size_groups = {}  # âœ å…ˆä¾å¤§å°åˆ†çµ„
    file_list = []
    total_size = 0
    duplicate_size = 0
    duplicates = []

    # æ”¶é›†æ‰€æœ‰æª”æ¡ˆ
    for root, _, files in os.walk(folder):
        for filename in files:
            ext = os.path.splitext(filename)[1].lower()
            if ext in ALL_EXTS:
                full_path = os.path.join(root, filename)
                try:
                    size = os.path.getsize(full_path)
                    total_size += size
                    file_list.append((full_path, size))
                    size_groups.setdefault(size, []).append(full_path)  # åˆ†çµ„
                except Exception as e:
                    print(f"âŒ ç„¡æ³•å–å¾—å¤§å° {full_path}: {e}")

    # å°åŒå¤§å°çš„æª”æ¡ˆç¾¤çµ„é€²è¡Œé›œæ¹Šæ¯”å°
    for size, paths in tqdm(size_groups.items(), desc="è™•ç†é€²åº¦"):
        if len(paths) < 2:
            continue  # ä¸å¯èƒ½é‡è¤‡
        for path in paths:
            file_hash = get_file_hash(path)
            if file_hash in hashes:
                existing_path = hashes[file_hash]

                # æ¯”è¼ƒä¿®æ”¹æ™‚é–“
                try:
                    existing_mtime = os.path.getmtime(existing_path)
                    current_mtime = os.path.getmtime(path)
                except Exception as e:
                    print(f"âš ï¸ ç„¡æ³•å–å¾— mtimeï¼š{e}")
                    continue

                if current_mtime > existing_mtime:
                    print(f"âš ï¸ æ‰¾åˆ°é‡è¤‡ï¼š\n   åŸå§‹ï¼š{existing_path}\n   é‡è¤‡ï¼š{path}")
                    duplicates.append((existing_path, path, size))
                else:
                    print(f"âš ï¸ æ‰¾åˆ°é‡è¤‡ï¼š\n   åŸå§‹ï¼š{path}\n   é‡è¤‡ï¼š{existing_path}")
                    duplicates.append((path, existing_path, size))
                    hashes[file_hash] = path  # æ›´æ–°ç‚ºæ–°çš„åŸå§‹åŸºæº–
                duplicate_size += size
            else:
                hashes[file_hash] = path



# æ‰¾å‡ºå‰ 20 å¤§æª”æ¡ˆ
    top_20_largest = sorted(file_list, key=lambda x: x[1], reverse=True)[:20]

    stats = {
        "total_files": len(file_list),
        "total_size": total_size,
        "duplicate_count": len(duplicates),
        "duplicate_size": duplicate_size
    }

    return duplicates, top_20_largest, stats


def write_output(duplicates, largest_files, stats, timestamp):
    dup_file = f'{timestamp}_duplicates_files.txt'
    size_file = f'{timestamp}_largest_files.txt'

    # å°‡é‡è¤‡æª”æ¡ˆä¾ç…§ hash åˆ†çµ„
    grouped = {}
    for original, duplicate, size in duplicates:
        key = original  # ç”¨åŸå§‹æª”æ¡ˆç•¶ä½œåˆ†çµ„ key
        if key not in grouped:
            grouped[key] = []
        grouped[key].append((duplicate, size))

    with open(dup_file, 'w', encoding='utf-8') as f:
        f.write("ğŸ“ é‡è¤‡æª”æ¡ˆæ¸…å–®ï¼š\n\n")
        for original, dup_list in grouped.items():
            f.write(f"ğŸŸ¡ åŸå§‹æª”æ¡ˆ: {original}\n")
            for duplicate, size in dup_list:
                f.write(f"    ğŸ” é‡è¤‡æª”æ¡ˆ: {duplicate}\n")
                f.write(f"       æª”æ¡ˆå¤§å°: {size / (1024 * 1024):.2f} MB\n")
            f.write("------------------------------------------------------------\n")
    print(f"âœ… å·²å¯«å…¥ {len(duplicates)} ç­†é‡è¤‡è³‡æ–™åˆ° {dup_file}")

    with open(size_file, 'w', encoding='utf-8') as f:
        f.write("ğŸ“Š çµ±è¨ˆè³‡è¨Š\n")
        f.write(f"æƒææª”æ¡ˆç¸½æ•¸: {stats['total_files']}\n")
        f.write(f"æƒæç¸½å¤§å°: {stats['total_size'] / (1024 * 1024):.2f} MB\n")
        f.write(f"é‡è¤‡æª”æ¡ˆæ•¸é‡: {stats['duplicate_count']}\n")
        f.write(f"å¯é‡‹æ”¾ç©ºé–“ç¸½è¨ˆ: {stats['duplicate_size'] / (1024 * 1024):.2f} MB\n")
        f.write("\nğŸ“ å‰ 20 å¤§æª”æ¡ˆï¼š\n")
        for path, size in largest_files:
            f.write(f"{path} ({size / (1024 * 1024):.2f} MB)\n")
    print(f"âœ… å·²å¯«å…¥å‰ 20 å¤§æª”æ¡ˆèˆ‡çµ±è¨ˆè³‡è¨Šåˆ° {size_file}")

if __name__ == "__main__":
    folder_to_scan = filedialog.askdirectory(title="è«‹é¸æ“‡è¦æƒæçš„è³‡æ–™å¤¾")

    if not os.path.isdir(folder_to_scan):
        print("âŒ æŒ‡å®šçš„è·¯å¾‘ä¸å­˜åœ¨æˆ–ä¸æ˜¯è³‡æ–™å¤¾ã€‚")
    else:
        timestamp = datetime.now().strftime('%m%d_%H%M')
        dup_files, largest_files, stats = find_duplicates_and_largest(folder_to_scan)
        write_output(dup_files, largest_files, stats, timestamp)
